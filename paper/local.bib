% This file was created with JabRef 2.9b2.
% Encoding: ISO8859_1

@ARTICLE{ArmaganDunsonLee2011,
  author = {{Armagan}, A. and {Dunson}, D. and {Lee}, J.},
  title = {Generalized double Pareto shrinkage},
  year = {2011},
  month = apr,
  eprint = {1104.0861},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1104.0861A},
  archiveprefix = {arXiv},
  journal = {ArXiv e-prints},
  keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Statistics
	- Machine Learning},
  owner = {jrnold},
  primaryclass = {stat.ME},
  timestamp = {2013.04.24}
}

@ARTICLE{BaeMallick2004,
  author = {Bae, Kyounghwa and Mallick, Bani K.},
  title = {Gene selection using a two-level hierarchical Bayesian model},
  year = {2004},
  volume = {20},
  number = {18},
  pages = {3423-3430},
  doi = {10.1093/bioinformatics/bth419},
  eprint = {http://bioinformatics.oxfordjournals.org/content/20/18/3423.full.pdf+html},
  url = {http://bioinformatics.oxfordjournals.org/content/20/18/3423.abstract},
  abstract = {Summary: The fundamental problem of gene selection via cDNA data is
	to identify which genes are differentially expressed across different
	kinds of tissue samples (e.g. normal and cancer). cDNA data contain
	large number of variables (genes) and usually the sample size is
	relatively small so the selection process can be unstable. Therefore,
	models which incorporate sparsity in terms of variables (genes) are
	desirable for this kind of problem. This paper proposes a two-level
	hierarchical Bayesian model for variable selection which assumes
	a prior that favors sparseness. We adopt a Markov chain Monte Carlo
	(MCMC) based computation technique to simulate the parameters from
	the posteriors. The method is applied to leukemia data from a previous
	study and a published dataset on breast cancer.Supplementary information:
	http://stat.tamu.edu/people/faculty/bmallick.html},
  journal = {Bioinformatics},
  owner = {jrnold}
}

@ARTICLE{Berger1980,
  author = {Berger, James},
  title = {A Robust Generalized Bayes Estimator and Confidence Region for a
	Multivariate Normal Mean},
  year = {1980},
  language = {English},
  volume = {8},
  number = {4},
  pages = {pp. 716-761},
  issn = {00905364},
  url = {http://www.jstor.org/stable/2240763},
  abstract = {It is observed that in selecting an alternative to the usual maximum
	likelihood estimator, ?0, of a multivariate normal mean, it is important
	to take into account prior information. Prior information in the
	form of a prior mean and a prior covariance matrix is considered.
	A generalized Bayes estimator is developed which is significantly
	better than ?0 if this prior information is correct and yet is very
	robust with respect to misspecification of the prior information.
	An associated confidence region is also constructed, and is shown
	to have very attractive size and probability of coverage.},
  copyright = {Copyright © 1980 Institute of Mathematical Statistics},
  journal = {The Annals of Statistics},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Jul., 1980},
  owner = {jrnold},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2013.04.24}
}

@REPORT{BrownGriffin2005,
  author = {Philip J. Brown and Jim E. Griffin},
  title = {Alternative Prior Distributions for Variable Selection with Very
	Many More Variables than Observations},
  type = {Technical Report},
  institution = {University of Warwick},
  year = {2005},
  abstract = {The problem of variable selection in regression and the generalised
	linear model is addressed. We adopt a Bayesian approach with priors
	for the regression coefficients that are scale mixtures of normal
	distributions and embody a high prior probability of proximity to
	zero. By seeking modal estimates we generalise the lasso. Properties
	of the priors and their resultant posteriors are explored in the
	context of the linear and generalised linear model especially when
	there are more variables than observations. We develop EM algorithms
	that embrace the need to explore the multiple modes of the non log-concave
	posterior distributions. Finally we apply the technique to microarray
	data using a probit model to find the genetic predictors of osteo-
	versus rheumatoid arthritis.},
  keywords = {Bayesian modal analysis, Variable selection in regression, Scale mixtures
	of normals, Improper Jeffreys prior, lasso, Penalised likelihood,
	EMalgorithm, Multiple modes, More variables than observations, Singular
	value decomposition, Latent variables, Probit regression.},
  owner = {jrnold},
  timestamp = {2013.04.24},
  url = {http://wrap.warwick.ac.uk/35585/}
}

@INPROCEEDINGS{CaronDoucet2008,
  author = {Caron, Fran\c{c}ois and Doucet, Arnaud},
  title = {Sparse Bayesian nonparametric regression},
  booktitle = {Proceedings of the 25th international conference on Machine learning},
  year = {2008},
  series = {ICML '08},
  publisher = {ACM},
  location = {Helsinki, Finland},
  isbn = {978-1-60558-205-4},
  pages = {88--95},
  doi = {10.1145/1390156.1390168},
  url = {http://doi.acm.org/10.1145/1390156.1390168},
  abstract = {One of the most common problems in machine learning and statistics
	consists of estimating the mean response X? from a vector of observations
	y assuming y = X? + ? where X is known, ? is a vector of parameters
	of interest and ? a vector of stochastic errors. We are particularly
	interested here in the case where the dimension K of ? is much higher
	than the dimension of y. We propose some flexible Bayesian models
	which can yield sparse estimates of ?. We show that as K ? ? these
	models are closely related to a class of Lévy processes. Simulations
	demonstrate that our models outperform significantly a range of popular
	alternatives.},
  acmid = {1390168},
  numpages = {8},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{CarvalhoPolsonScott2010,
  author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
  title = {The horseshoe estimator for sparse signals},
  year = {2010},
  volume = {97},
  number = {2},
  pages = {465-480},
  doi = {10.1093/biomet/asq017},
  eprint = {http://biomet.oxfordjournals.org/content/97/2/465.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/97/2/465.abstract},
  abstract = {This paper proposes a new approach to sparsity, called the horseshoe
	estimator, which arises from a prior based on multivariate-normal
	scale mixtures. We describe the estimator?s advantages over existing
	approaches, including its robustness, adaptivity to different sparsity
	patterns and analytical tractability. We prove two theorems: one
	that characterizes the horseshoe estimator?s tail robustness and
	the other that demonstrates a super-efficient rate of convergence
	to the correct estimate of the sampling density in sparse situations.
	Finally, using both real and simulated data, we show that the horseshoe
	estimator corresponds quite closely to the answers obtained by Bayesian
	model averaging under a point-mass mixture prior.},
  journal = {Biometrika},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{CarvalhoPolsonScott2009,
  author = {Carlos M. Carvalho and Nicholas G. Polson and James G. Scott},
  title = {Handling Sparsity via the Horseshoe},
  journaltitle = {Journal of Machine Learning and Research: Workshop and Conference
	Proceedings},
  year = {2009},
  volume = {5},
  pages = {73-80},
  abstract = {This paper presents a general, fully Bayesian framework for sparse
	supervised-learning problems based on the horseshoe prior. The horseshoe
	prior is a member of the family of multivariate scale mixtures of
	normals, and is therefore closely related to widely used approaches
	for sparse Bayesian learning, including, among others, Laplacian
	priors (e.g. the LASSO) and Student-t priors (e.g. the relevance
	vector machine). The advantages of the horseshoe are its robustness
	at handling unknown sparsity and large outlying signals. These properties
	are justifed theoretically via a representation theorem and accompanied
	by comprehensive empirical experiments that compare its performance
	to benchmark alternatives.},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{Cobb1978,
  author = {Cobb, George W.},
  title = {The problem of the Nile: Conditional solution to a changepoint problem},
  year = {1978},
  volume = {65},
  number = {2},
  pages = {243-251},
  doi = {10.1093/biomet/65.2.243},
  eprint = {http://biomet.oxfordjournals.org/content/65/2/243.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/65/2/243.abstract},
  abstract = {Inference is considered for the point in a sequence of random variables
	at which the probability distribution changes. An approximation to
	the conditional distribution of the maximum likelihood estimator
	of the changepoint given the ancillary values of observations adjacent
	to the estimated changepoint is derived and shown to be numerically
	equal to a Bayesian posterior distribution for the changepoint. A
	hydrological example is given to show that inferences based on the
	conditional distribution of the maximum likelihood estimator can
	differ sharply from inferences based on the marginal distribution},
  journal = {Biometrika},
  owner = {jeff},
  timestamp = {2013.04.22}
}

@BOOK{CommandeurKoopman2007,
  author = {Commandeur, Jacques J.F. and Koopman, Siem Jan},
  title = {An Introduction to State Space Time Series Analysis},
  year = {2007},
  series = {Practical Econometrics Series},
  publisher = {OUP Oxford},
  isbn = {9780191607806},
  url = {http://books.google.com/books?id=OCmljPYfgkUC},
  file = {CommandeurKoopman2007.pdf:CommandeurKoopman2007.pdf:PDF},
  owner = {jrnold}
}

@BOOK{DurbinKoopman2012,
  author = {Durbin, J. and Koopman, S.J.},
  title = {Time Series Analysis by State Space Methods: Second Edition},
  year = {2012},
  series = {Oxford Statistical Science Series},
  publisher = {OUP Oxford},
  isbn = {9780199641178},
  url = {http://books.google.com/books?id=fOq39Zh0olQC},
  lccn = {2011945385},
  owner = {jrnold},
  timestamp = {2013.04.22}
}

@ARTICLE{FigueiredoMember2003,
  author = {Mario A.T. Figueiredo and Senior Member},
  title = {Adaptive Sparseness for Supervised Learning},
  year = {2003},
  volume = {25},
  pages = {1150--1159},
  abstract = {The goal of supervised learning is to infer a functional mapping based
	on a set of training examples. To achieve good generalization, it
	is necessary to control the "complexity" of the learned function.
	In Bayesian approaches, this is done by adopting a prior for the
	parameters of the function being learned. We propose a Bayesian approach
	to supervised learning, which leads to sparse solutions; that is,
	in which irrelevant parameters are automatically set exactly to zero.
	Other ways to obtain sparse classifiers (such as Laplacian priors,
	support vector machines) involve (hyper)parameters which control
	the degree of sparseness of the resulting classifiers; these parameters
	have to be somehow adjusted/estimated from the training data. In
	contrast, our approach does not involve any (hyper)parameters to
	be adjusted or estimated. This is achieved by a hierarchical-Bayes
	interpretation of the Laplacian prior, which is then modified by
	the adoption of a Jeffreys' noninformative hyperprior. Implementation
	is carried out by an expectationmaximization (EM) algorithm. Experiments
	with several benchmark data sets show that the proposed approach
	yields state-of-the-art performance. In particular, our method outperforms
	SVMs and performs competitively with the best alternative techniques,
	although it involves no tuning or adjustment of sparseness-controlling
	hyperparameters.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{GiordaniKohn2008,
  author = {Giordani, Paolo and Kohn, Robert},
  title = {Efficient Bayesian Inference for Multiple Change-Point and Mixture
	Innovation Models},
  year = {2008},
  volume = {26},
  number = {1},
  pages = {66-77},
  doi = {10.1198/073500107000000241},
  eprint = {http://amstat.tandfonline.com/doi/pdf/10.1198/073500107000000241},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/073500107000000241},
  abstract = { Time series subject to parameter shifts of random magnitude and timing
	are commonly modeled with a change-point approach using Chib's algorithm
	to draw the break dates. We outline some advantages of an alternative
	approach in which breaks come through mixture distributions in state
	innovations, and for which the sampler of Gerlach, Carter, and Kohn
	allows reliable and efficient inference. We show how the same sampler
	can be used to model shifts in variance that occur independently
	of shifts in other parameters and how to draw the break dates efficiently
	when regime durations follow a Poisson process. Finally, we introduce
	to the time series literature the concept of adaptive Metropolisâ€“Hastings
	sampling for discrete latent variable models. We develop an easily
	implemented adaptive algorithm that improves on the work of Gerlach
	et al. and promises to significantly reduce computing time in a variety
	of problems including mixture innovation, change-point, regime switching,
	and outlier detection. The efficiency gains on two models for U.S.
	inflation and real interest rates are 257% and 341%. },
  journal = {Journal of Business \& Economic Statistics},
  owner = {jeff},
  timestamp = {2013.04.22}
}

@ARTICLE{Hans2009,
  author = {Hans, Chris},
  title = {Bayesian lasso regression},
  year = {2009},
  volume = {96},
  number = {4},
  pages = {835-845},
  doi = {10.1093/biomet/asp047},
  eprint = {http://biomet.oxfordjournals.org/content/96/4/835.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/96/4/835.abstract},
  abstract = {The lasso estimate for linear regression corresponds to a posterior
	mode when independent, double-exponential prior distributions are
	placed on the regression coefficients. This paper introduces new
	aspects of the broader Bayesian treatment of lasso regression. A
	direct characterization of the regression coefficients? posterior
	distribution is provided, and computation and inference under this
	characterization is shown to be straightforward. Emphasis is placed
	on point estimation using the posterior mean, which facilitates prediction
	of future observations via the posterior predictive distribution.
	It is shown that the standard lasso prediction method does not necessarily
	agree with model-based, Bayesian predictions. A new Gibbs sampler
	for Bayesian lasso regression is introduced.},
  journal = {Biometrika},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{HarveyKoopman2000,
  author = {Harvey, Andrew and Koopman, Siem Jan},
  title = {Signal extraction and the formulation of unobserved components models},
  year = {2000},
  volume = {3},
  number = {1},
  pages = {84--107},
  issn = {1368-423X},
  doi = {10.1111/1368-423X.00040},
  url = {http://dx.doi.org/10.1111/1368-423X.00040},
  abstract = {This paper looks at unobserved components models and examines the
	implied weighting patterns for signal extraction. There are four
	main themes. The first concerns the implications of correlated disturbances
	driving the components, especially those cases in which the correlation
	is perfect. The second is about the way in which ARIMA-based methods
	for trend extraction relate to those based on unobserved components.
	The third explores the impact of heteroscedasticity and irregular
	spacing and shows how setting up models with t-distributed disturbances
	leads to weighting patterns which are robust to outliers and breaks.
	Finally, a comparison is made between implied weighting patterns
	with kernels used in non-parametric trend estimation and equivalent
	kernels used in spline smoothing. It is demonstrated that with irregularly
	spaced data, the weighting used by conventional spline smoothing
	techniques is not the same as that obtained from the time series
	model based approach.},
  journal = {Econometrics Journal},
  keywords = {Cubic splines, Kalman filter and smoother, Kernels, Robustness, Structural
	time series model, Trend, Wienerâ€“Kolmogorov filter.},
  owner = {jeff},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2013.04.22}
}

@ARTICLE{LiGoel2006,
  author = {Bin Li and Prem K. Goel},
  title = {Regularized Optimization in Statistical Learning: A Bayesian Perspective},
  journaltitle = {Statistica Sinica},
  year = {2006},
  number = {2},
  issue = {16},
  pages = {411-424},
  abstract = {Regularization plays a major role in modern data analysis, whenever
	non-regularized fitting is likely to lead to over-fitted model. It
	is known that most regularized optimization problems have Bayesian
	interpretation in which the prior plays the role of the regularizer.
	In this paper, we consider the issue of sensitivity of the regularized
	solution to the prior specification within the Bayesian perspective.
	We suggest a class of flat-tailed priors for a general likelihood
	function for robust Bayesian solutions, in the same spirit as the
	-distribution being suggested as a flat-tail prior for normal likelihood.
	Results are applied to a family of regularized learning methods and
	group LASSO. In addition, the consistency issue for LASSO is discussed
	within this framework.},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{ParkCasella2008,
  author = {Park, Trevor and Casella, George},
  title = {The Bayesian Lasso},
  year = {2008},
  volume = {103},
  number = {482},
  pages = {681-686},
  doi = {10.1198/016214508000000337},
  eprint = {http://amstat.tandfonline.com/doi/pdf/10.1198/016214508000000337},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337},
  abstract = { The Lasso estimate for linear regression parameters can be interpreted
	as a Bayesian posterior mode estimate when the regression parameters
	have independent Laplace (i.e., double-exponential) priors. Gibbs
	sampling from this posterior is possible using an expanded hierarchy
	with conjugate normal priors for the regression parameters and independent
	exponential priors on their variances. A connection with the inverse-Gaussian
	distribution provides tractable full conditional distributions. The
	Bayesian Lasso provides interval estimates (Bayesian credible intervals)
	that can guide variable selection. Moreover, the structure of the
	hierarchical model provides both Bayesian and likelihood methods
	for selecting the Lasso parameter. Slight modifications lead to Bayesian
	versions of other Lasso-related estimation methods, including bridge
	regression and a robust variant. },
  journal = {Journal of the American Statistical Association},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@BOOK{PetrisPetroneEtAl2009,
  author = {Petris, G. and Petrone, S. and Campagnoli, P.},
  title = {{Dynamic Linear Models with R}},
  year = {2009},
  series = {Use R!},
  publisher = {Springer},
  isbn = {9780387772370},
  url = {http://books.google.com/books?id=VCt3zVq8TO8C},
  file = {PetrisPetroneEtAl2009.pdf:PetrisPetroneEtAl2009.pdf:PDF},
  lccn = {2009926480},
  owner = {jrnold},
  timestamp = {2010.11.10}
}

@ARTICLE{PolsonScott2010,
  author = {Nicholas G. Polson and James G. Scott},
  title = {Shrink Globally, Act Locally: Sparse Bayesian Regularization and
	Prediction},
  journaltitle = {Bayesian Statistics},
  year = {2010},
  abstract = {We use Levy processes to generate joint prior distributions for a
	location
	
	parameter ? = (?1 , . . . , ?p ) as p grows large. This approach,
	which generalizes
	
	normal scale-mixture priors to an infinite-dimensional setting, has
	a number
	
	of connections with mathematical finance and Bayesian nonparametrics.
	We
	
	argue that it provides an intuitive framework for generating new regularization
	
	penalties and shrinkage rules; for performing asymptotic analysis
	on existing
	
	models; and for simplifying proofs of some classic results on normal
	scale
	
	mixtures.},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{RatkovicEng2010,
  author = {Ratkovic, Marc T. and Eng, Kevin H.},
  title = {Finding Jumps in Otherwise Smooth Curves: Identifying Critical Events
	in Political Processes},
  year = {2010},
  volume = {18},
  number = {1},
  pages = {57-77},
  doi = {10.1093/pan/mpp032},
  eprint = {http://pan.oxfordjournals.org/content/18/1/57.full.pdf+html},
  url = {http://pan.oxfordjournals.org/content/18/1/57.abstract},
  abstract = {Many social processes are stable and smooth in general, with discrete
	jumps. We develop a sequential segmentation spline method that can
	identify both the location and the number of discontinuities in a
	series of observations with a time component, while fitting a smooth
	spline between jumps, using a modified Bayesian Information Criterion
	statistic as a stopping rule. We explore the method in a large-n,
	unbalanced panel setting with George W. Bush's approval data, a small-n
	time series with median DW-NOMINATE scores for each Congress over
	time, and a series of simulations. We compare the method to several
	extant smoothers, and the method performs favorably in terms of visual
	inspection, residual properties, and event detection. Finally, we
	discuss extensions of the method.},
  journal = {Political Analysis},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@BOOK{ShumwayStoffer2010,
  author = {Shumway, R.H. and Stoffer, D.S.},
  title = {Time Series Analysis and Its Applications},
  year = {2010},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  isbn = {9781441978653},
  url = {http://books.google.com/books?id=NIhXa6UeF2cC},
  owner = {jrnold},
  timestamp = {2013.02.01}
}

@ARTICLE{Strawderman1971,
  author = {Strawderman, William E.},
  title = {Proper Bayes Minimax Estimators of the Multivariate Normal Mean},
  year = {1971},
  language = {English},
  volume = {42},
  number = {1},
  pages = {pp. 385-388},
  issn = {00034851},
  url = {http://www.jstor.org/stable/2958496},
  copyright = {Copyright © 1971 Institute of Mathematical Statistics},
  journal = {The Annals of Mathematical Statistics},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Feb., 1971},
  owner = {jrnold},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2013.04.24}
}

@ARTICLE{Tipping2001,
  author = {Tipping, Michael E.},
  title = {Sparse bayesian learning and the relevance vector machine},
  year = {2001},
  volume = {1},
  month = sep,
  pages = {211--244},
  issn = {1532-4435},
  doi = {10.1162/15324430152748236},
  url = {http://dx.doi.org/10.1162/15324430152748236},
  abstract = {This paper introduces a general Bayesian framework for obtaining sparse
	solutions to regression and classification tasks utilising models
	linear in the parameters. Although this framework is fully general,
	we illustrate our approach with a particular specialisation that
	we denote the 'relevance vector machine' (RVM), a model of identical
	functional form to the popular and state-of-the-art 'support vector
	machine' (SVM). We demonstrate that by exploiting a probabilistic
	Bayesian learning framework, we can derive accurate prediction models
	which typically utilise dramatically fewer basis functions than a
	comparable SVM while offering a number of additional advantages.
	These include the benefits of probabilistic predictions, automatic
	estimation of 'nuisance' parameters, and the facility to utilise
	arbitrary basis functions (e.g. non-'Mercer' kernels). We detail
	the Bayesian framework and associated learning algorithm for the
	RVM, and give some illustrative examples of its application along
	with some comparative benchmarks. We offer some explanation for the
	exceptional degree of sparsity obtained, and discuss and demonstrate
	some of the advantageous features, and potential extensions, of Bayesian
	relevance learning.},
  acmid = {944741},
  issue_date = {9/1/2001},
  journal = {J. Mach. Learn. Res.},
  numpages = {34},
  owner = {jrnold},
  publisher = {JMLR.org},
  timestamp = {2013.04.24}
}

@BOOK{WestHarrison1997,
  author = {West, M. and Harrison, J.},
  title = {{Bayesian forecasting and dynamic models}},
  year = {1997},
  series = {Springer series in statistics},
  publisher = {Springer},
  isbn = {9780387947259},
  url = {http://books.google.com/books?id=jcl8lD75fkYC},
  file = {west1997bayesian.pdf:west1997bayesian.pdf:PDF},
  lccn = {96038166},
  owner = {jrnold},
  timestamp = {2010.11.19}
}

